{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08d861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Iterable\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        defaults = {\"lr\": lr}\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure: Callable | None = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"] # Get the learning rate\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p] # Get state associated with p.\n",
    "                t = state.get(\"t\", 0) # Get iteration number from the state, or initial value.\n",
    "                grad = p.grad.data # Get the gradient of loss with respect to p.\n",
    "                p.data -= lr / math.sqrt(t + 1) * grad # Update weight tensor in-place.\n",
    "                state[\"t\"] = t + 1 # Increment iteration number.\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe11085",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 5 * torch.randn((10, 10))\n",
    "def run_training_loop(weights: torch.nn.Parameter, lr: float, num_iter:int=10):\n",
    "    opt = SGD([weights], lr=lr)\n",
    "    for t in range(num_iter):\n",
    "        opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "        loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "        print(f\"LR: {lr}, Iter {num_iter}: {loss.cpu().item()}\")\n",
    "        loss.backward() # Run backward pass, which computes gradients.\n",
    "        opt.step() # Run optimizer step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca1e6a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 1, Iter 10: 20.35976219177246\n",
      "LR: 1, Iter 10: 19.553516387939453\n",
      "LR: 1, Iter 10: 19.004369735717773\n",
      "LR: 1, Iter 10: 18.568017959594727\n",
      "LR: 1, Iter 10: 18.198514938354492\n",
      "LR: 1, Iter 10: 17.874425888061523\n",
      "LR: 1, Iter 10: 17.583728790283203\n",
      "LR: 1, Iter 10: 17.318893432617188\n",
      "LR: 1, Iter 10: 17.074832916259766\n",
      "LR: 1, Iter 10: 16.84792709350586\n"
     ]
    }
   ],
   "source": [
    "run_training_loop(torch.nn.Parameter(weights.clone()), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd41b7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 10, Iter 10: 20.35976219177246\n",
      "LR: 10, Iter 10: 13.030248641967773\n",
      "LR: 10, Iter 10: 9.605342864990234\n",
      "LR: 10, Iter 10: 7.515154838562012\n",
      "LR: 10, Iter 10: 6.087275505065918\n",
      "LR: 10, Iter 10: 5.047048568725586\n",
      "LR: 10, Iter 10: 4.256515979766846\n",
      "LR: 10, Iter 10: 3.637314558029175\n",
      "LR: 10, Iter 10: 3.1411068439483643\n",
      "LR: 10, Iter 10: 2.736253261566162\n"
     ]
    }
   ],
   "source": [
    "run_training_loop(torch.nn.Parameter(weights.clone()), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9d4d104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 100, Iter 10: 20.35976219177246\n",
      "LR: 100, Iter 10: 20.35976219177246\n",
      "LR: 100, Iter 10: 3.493182420730591\n",
      "LR: 100, Iter 10: 0.08359973132610321\n",
      "LR: 100, Iter 10: 8.72220332208744e-17\n",
      "LR: 100, Iter 10: 9.72143704710216e-19\n",
      "LR: 100, Iter 10: 3.273549825837561e-20\n",
      "LR: 100, Iter 10: 1.9500762746098454e-21\n",
      "LR: 100, Iter 10: 1.6729012234271768e-22\n",
      "LR: 100, Iter 10: 1.8587793825647003e-23\n"
     ]
    }
   ],
   "source": [
    "run_training_loop(torch.nn.Parameter(weights.clone()), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad11e752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 1000, Iter 10: 20.35976219177246\n",
      "LR: 1000, Iter 10: 7349.87451171875\n",
      "LR: 1000, Iter 10: 1269438.875\n",
      "LR: 1000, Iter 10: 141211520.0\n",
      "LR: 1000, Iter 10: 11438132224.0\n",
      "LR: 1000, Iter 10: 721877139456.0\n",
      "LR: 1000, Iter 10: 37058814935040.0\n",
      "LR: 1000, Iter 10: 1594428762357760.0\n",
      "LR: 1000, Iter 10: 5.876724294221824e+16\n",
      "LR: 1000, Iter 10: 1.887081361391485e+18\n"
     ]
    }
   ],
   "source": [
    "run_training_loop(torch.nn.Parameter(weights.clone()), 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f138d50",
   "metadata": {},
   "source": [
    "Observation of above results: the higher the learning rate, the faster the rate of decrease in loss. \n",
    "However, if the learning rate is too high, we will overshoot the minima and end up oscillating, which is what we observe with a lr of 1000."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
